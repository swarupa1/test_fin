# This Jupyter notebook contains the development and testing code for the chatbot, including data loading, model initialization, and interaction logic.

import numpy as np
import pandas as pd
import os
from langchain.docstore.document import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import torch
import transformers
from transformers import AutoTokenizer

# Load Dataset
bank = pd.read_csv("BankFAQs.csv")
bank["content"] = bank.apply(lambda row: f"Question: {row['Question']}\nAnswer: {row['Answer']}", axis=1)

# Prepare documents for LangChain
documents = []
for _, row in bank.iterrows():
    documents.append(Document(page_content=row["content"], metadata={"class": row["Class"]}))

# Initialize embeddings and vector store
hg_embeddings = HuggingFaceEmbeddings()
persist_directory = '/content/my_new_directory'
langchain_chroma = Chroma.from_documents(
    documents=documents,
    collection_name="chatbot_finance",
    embedding=hg_embeddings,
    persist_directory=persist_directory
)

# Model configuration
model_id = '*********'
device = 'cuda' if torch.cuda.is_available() else 'cpu'
bnb_config = transformers.BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16
)

model_config = transformers.AutoConfig.from_pretrained(
    model_id,
    trust_remote_code=True,
    max_new_tokens=1024
)

model = transformers.AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    config=model_config,
    quantization_config=bnb_config,
    device_map='auto'
)

# Initialize the query pipeline
query_pipeline = transformers.pipeline(
    "text-generation",
    model=model,
    tokenizer=AutoTokenizer.from_pretrained(model_id),
    torch_dtype=torch.float16,
    max_length=6000,
    max_new_tokens=500,
    device_map="auto"
)

# Define the prompt template for the QA chain
template = """
You are a Finance QNA Expert, Analyze the Query and Respond to Customer with suitable answer. If you don't know the answer, just say "Sorry, I don't know."
Question: {question}
Context: {context}
Answer:
"""
PROMPT = PromptTemplate(input_variables=["context", "query"], template=template)

retriever = langchain_chroma.as_retriever(search_kwargs={"k": 1})

qa_chain = RetrievalQA.from_chain_type(
    HuggingFacePipeline(pipeline=query_pipeline), 
    retriever=retriever, 
    chain_type_kwargs={"prompt": PROMPT}
)

# Chat function
def chat_with_rag():
    print("Welcome to the GenAI Financial Chatbot. Type 'exit' to end the conversation.")
    while True:
        query = input("You: ")
        if query.lower() in ["exit", "quit"]:
            break
        context = "Your context here"  # Provide context if necessary, otherwise leave it empty
        try:
            result = qa_chain({"context": context, "query": query})
            print(f"Chatbot: {result['result']}")
        except RuntimeError as e:
            print(f"RuntimeError encountered: {e}")

# Run the chat
chat_with_rag()
